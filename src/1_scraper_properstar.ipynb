{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11e7100a-7153-4098-92dd-4e19cfcc41d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d394befa-7b94-4bca-95e4-503ea0575f61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%pip install playwright nest_asyncio plotly pandas geopy beautifulsoup4 scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f051cd5-92ce-4c72-8aa0-c0f14b944f58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Browser Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5198cfc-bace-4140-8da2-d41a37fe1cf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Chromium 143.0.7499.4 (playwright build v1200)\u001B[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/chromium/1200/chromium-linux.zip\u001B[22m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(node:1731) [DEP0169] DeprecationWarning: `url.parse()` behavior is not standardized and prone to errors that have security implications. Use the WHATWG URL API instead. CVEs are not issued for `url.parse()` vulnerabilities.\n(Use `node --trace-deprecation ...` to show where the warning was created)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|                                                                                |   0% of 164.7 MiB\n|■■■■■■■■                                                                        |  10% of 164.7 MiB\n|■■■■■■■■■■■■■■■■                                                                |  20% of 164.7 MiB\n|■■■■■■■■■■■■■■■■■■■■■■■■                                                        |  30% of 164.7 MiB\n|■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■                                                |  40% of 164.7 MiB\n|■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■                                        |  50% of 164.7 MiB\n|■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■                                |  60% of 164.7 MiB\n|■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■                        |  70% of 164.7 MiB\n|■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■                |  80% of 164.7 MiB\n|■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■        |  90% of 164.7 MiB\n|■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■| 100% of 164.7 MiB\nChromium 143.0.7499.4 (playwright build v1200) downloaded to /root/.cache/ms-playwright/chromium-1200\nDownloading FFMPEG playwright build v1011\u001B[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/ffmpeg/1011/ffmpeg-linux.zip\u001B[22m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(node:1831) [DEP0169] DeprecationWarning: `url.parse()` behavior is not standardized and prone to errors that have security implications. Use the WHATWG URL API instead. CVEs are not issued for `url.parse()` vulnerabilities.\n(Use `node --trace-deprecation ...` to show where the warning was created)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|                                                                                |   0% of 2.3 MiB\n|■■■■■■■■                                                                        |  10% of 2.3 MiB\n|■■■■■■■■■■■■■■■■                                                                |  20% of 2.3 MiB\n|■■■■■■■■■■■■■■■■■■■■■■■■                                                        |  30% of 2.3 MiB\n|■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■                                                |  40% of 2.3 MiB\n|■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■                                        |  50% of 2.3 MiB\n|■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■                                |  60% of 2.3 MiB\n|■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■                        |  70% of 2.3 MiB\n|■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■                |  80% of 2.3 MiB\n|■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■        |  90% of 2.3 MiB\n|■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■| 100% of 2.3 MiB\nFFMPEG playwright build v1011 downloaded to /root/.cache/ms-playwright/ffmpeg-1011\nDownloading Chromium Headless Shell 143.0.7499.4 (playwright build v1200)\u001B[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/chromium/1200/chromium-headless-shell-linux.zip\u001B[22m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(node:1843) [DEP0169] DeprecationWarning: `url.parse()` behavior is not standardized and prone to errors that have security implications. Use the WHATWG URL API instead. CVEs are not issued for `url.parse()` vulnerabilities.\n(Use `node --trace-deprecation ...` to show where the warning was created)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|                                                                                |   0% of 109.7 MiB\n|■■■■■■■■                                                                        |  10% of 109.7 MiB\n|■■■■■■■■■■■■■■■■                                                                |  20% of 109.7 MiB\n|■■■■■■■■■■■■■■■■■■■■■■■■                                                        |  30% of 109.7 MiB\n|■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■                                                |  40% of 109.7 MiB\n|■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■                                        |  50% of 109.7 MiB\n|■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■                                |  60% of 109.7 MiB\n|■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■                        |  70% of 109.7 MiB\n|■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■                |  80% of 109.7 MiB\n|■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■        |  90% of 109.7 MiB\n|■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■| 100% of 109.7 MiB\nChromium Headless Shell 143.0.7499.4 (playwright build v1200) downloaded to /root/.cache/ms-playwright/chromium_headless_shell-1200\n"
     ]
    }
   ],
   "source": [
    "%sh\n",
    "playwright install chromium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e644b13e-45fa-492f-bae4-bdc3cae5a1b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Configuration & Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d24594c7-01f9-467c-bc31-5ee0d835b3b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Config Loaded. Targeting New York via Price Segmentation.\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Apply patches\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# --- 1. PROJECT SETTINGS ---\n",
    "TARGET_CITY = \"New York\"\n",
    "# We loop through price ranges to force the site to show us 3000+ items\n",
    "PRICE_RANGES = [\n",
    "    (0, 500000),\n",
    "    (500000, 900000),\n",
    "    (900000, 1500000),\n",
    "    (1500000, 3000000),\n",
    "    (3000000, 10000000)\n",
    "]\n",
    "\n",
    "# --- 2. AZURE STORAGE ---\n",
    "# Replace with your storage account name\n",
    "STORAGE_ACCOUNT = \"lab94290\" \n",
    "CONTAINER = \"airbnb\"\n",
    "OUTPUT_PATH = f\"abfss://{CONTAINER}@{STORAGE_ACCOUNT}.dfs.core.windows.net/student_submissions/nyc_arbitrage_final.csv\"\n",
    "\n",
    "# --- 3. CREDENTIALS (Loaded from secrets.json) ---\n",
    "import json\n",
    "import os\n",
    "try:\n",
    "    if os.path.exists('secrets.json'):\n",
    "        with open('secrets.json') as f: secrets = json.load(f)\n",
    "    elif os.path.exists('../secrets.json'):\n",
    "        with open('../secrets.json') as f: secrets = json.load(f)\n",
    "    else:\n",
    "        secrets = {}\n",
    "\n",
    "    BRIGHTDATA_USER = secrets.get('BRIGHTDATA_USER', 'PASTE_USER_HERE')\n",
    "    BRIGHTDATA_PASS = secrets.get('BRIGHTDATA_PASS', 'PASTE_PASS_HERE')\n",
    "    BRIGHTDATA_HOST = secrets.get('BRIGHTDATA_HOST', 'brd.superproxy.io:9222')\n",
    "except:\n",
    "    BRIGHTDATA_USER = 'PASTE_USER_HERE'\n",
    "    BRIGHTDATA_PASS = 'PASTE_PASS_HERE'\n",
    "    BRIGHTDATA_HOST = 'brd.superproxy.io:9222'\n",
    "\n",
    "print(f\"✅ Config Loaded. Targeting {TARGET_CITY} via Price Segmentation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6fd34706-515d-4df9-84cf-836c5c7dc8ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Phase 1 - The \"Volume\" Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a88b2dc-ac47-41ec-abd3-6498677f428b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\uD83D\uDE80 Phase 1: Finding Listings (5 Segments x 30 Pages)...\n\n\uD83D\uDCB5 Scanning Range: $0 - $500,000\n   PAGE 1: Found 21 items.\n   PAGE 4: Found 21 items.\n   PAGE 5: Found 21 items.\n   PAGE 6: Found 21 items.\n   PAGE 8: Found 21 items.\n   PAGE 9: Found 21 items.\n   PAGE 10: Found 21 items.\n   PAGE 11: Found 21 items.\n   PAGE 12: Found 21 items.\n   PAGE 13: Found 21 items.\n   PAGE 14: Found 21 items.\n   PAGE 16: Found 21 items.\n   PAGE 17: Found 21 items.\n   PAGE 18: Found 21 items.\n   PAGE 19: Found 21 items.\n   PAGE 20: Found 21 items.\n   PAGE 21: Found 21 items.\n   PAGE 22: Found 21 items.\n   PAGE 23: Found 21 items.\n   PAGE 24: Found 21 items.\n   PAGE 25: Found 21 items.\n   PAGE 26: Found 21 items.\n   PAGE 27: Found 21 items.\n   PAGE 28: Found 21 items.\n   PAGE 29: Found 21 items.\n   PAGE 30: Found 21 items.\n\n\uD83D\uDCB5 Scanning Range: $500,000 - $900,000\n   PAGE 1: Found 21 items.\n   PAGE 2: Found 21 items.\n   PAGE 3: Found 21 items.\n   PAGE 4: Found 21 items.\n   PAGE 5: Found 21 items.\n   PAGE 6: Found 21 items.\n   PAGE 7: Found 21 items.\n   PAGE 8: Found 21 items.\n   PAGE 9: Found 21 items.\n   PAGE 10: Found 21 items.\n   PAGE 11: Found 21 items.\n   PAGE 12: Found 21 items.\n   PAGE 13: Found 21 items.\n   PAGE 14: Found 21 items.\n   PAGE 15: Found 21 items.\n   PAGE 16: Found 21 items.\n   PAGE 18: Found 21 items.\n   PAGE 19: Found 21 items.\n   PAGE 20: Found 21 items.\n   PAGE 21: Found 21 items.\n   PAGE 22: Found 21 items.\n   PAGE 23: Found 21 items.\n   PAGE 24: Found 21 items.\n   PAGE 25: Found 21 items.\n   PAGE 26: Found 21 items.\n   PAGE 27: Found 21 items.\n   PAGE 28: Found 21 items.\n   PAGE 29: Found 21 items.\n\n\uD83D\uDCB5 Scanning Range: $900,000 - $1,500,000\n   PAGE 1: Found 21 items.\n   PAGE 2: Found 21 items.\n   PAGE 3: Found 21 items.\n   PAGE 4: Found 21 items.\n   PAGE 5: Found 21 items.\n   PAGE 6: Found 21 items.\n   PAGE 7: Found 21 items.\n   PAGE 8: Found 21 items.\n   PAGE 9: Found 21 items.\n   PAGE 10: Found 21 items.\n   PAGE 11: Found 21 items.\n   PAGE 12: Found 21 items.\n   PAGE 13: Found 21 items.\n   PAGE 14: Found 21 items.\n   PAGE 15: Found 21 items.\n   PAGE 16: Found 21 items.\n   PAGE 17: Found 21 items.\n   PAGE 18: Found 21 items.\n   PAGE 19: Found 21 items.\n   PAGE 28: Found 21 items.\n   PAGE 29: Found 21 items.\n   PAGE 30: Found 21 items.\n\n\uD83D\uDCB5 Scanning Range: $1,500,000 - $3,000,000\n   PAGE 1: Found 21 items.\n   PAGE 2: Found 21 items.\n   PAGE 3: Found 21 items.\n   PAGE 4: Found 21 items.\n   PAGE 5: Found 21 items.\n   PAGE 6: Found 21 items.\n   PAGE 7: Found 21 items.\n   PAGE 8: Found 21 items.\n   PAGE 9: Found 21 items.\n   PAGE 10: Found 21 items.\n   PAGE 11: Found 21 items.\n   PAGE 12: Found 21 items.\n   PAGE 13: Found 21 items.\n   PAGE 14: Found 21 items.\n   PAGE 15: Found 21 items.\n   PAGE 16: Found 21 items.\n   PAGE 17: Found 21 items.\n   PAGE 18: Found 21 items.\n   PAGE 19: Found 21 items.\n   PAGE 22: Found 21 items.\n   PAGE 23: Found 21 items.\n   PAGE 24: Found 21 items.\n   PAGE 25: Found 21 items.\n   PAGE 27: Found 21 items.\n   PAGE 28: Found 21 items.\n   PAGE 29: Found 21 items.\n   PAGE 30: Found 21 items.\n\n\uD83D\uDCB5 Scanning Range: $3,000,000 - $10,000,000\n   PAGE 1: Found 21 items.\n   PAGE 2: Found 21 items.\n   PAGE 3: Found 21 items.\n   PAGE 4: Found 21 items.\n   PAGE 5: Found 21 items.\n   PAGE 6: Found 21 items.\n   PAGE 7: Found 21 items.\n   PAGE 8: Found 21 items.\n   PAGE 9: Found 21 items.\n   PAGE 10: Found 21 items.\n   PAGE 11: Found 21 items.\n   PAGE 12: Found 21 items.\n   PAGE 14: Found 21 items.\n   PAGE 15: Found 21 items.\n   PAGE 16: Found 21 items.\n   PAGE 18: Found 21 items.\n   PAGE 20: Found 21 items.\n   PAGE 21: Found 21 items.\n   PAGE 22: Found 21 items.\n   PAGE 23: Found 21 items.\n   PAGE 24: Found 21 items.\n   PAGE 25: Found 21 items.\n   PAGE 27: Found 21 items.\n   PAGE 30: Found 21 items.\n\n\uD83C\uDF89 FOUND 1755 LISTINGS.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "async def scrape_listing_urls():\n",
    "    sbr_connection = f'wss://{BRIGHTDATA_USER}:{BRIGHTDATA_PASS}@{BRIGHTDATA_HOST}'\n",
    "    base_url = \"https://www.properstar.com/united-states/new-york/buy/apartment-house\"\n",
    "    \n",
    "    collected_data = []\n",
    "    pages_per_segment = 30  # Adjust: 30 pages * 5 segments * 20 items = ~3000 items\n",
    "    \n",
    "    print(f\"\uD83D\uDE80 Phase 1: Finding Listings ({len(PRICE_RANGES)} Segments x {pages_per_segment} Pages)...\")\n",
    "\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.connect_over_cdp(sbr_connection)\n",
    "        try:\n",
    "            context = await browser.new_context(locale='en-US')\n",
    "            page = await context.new_page()\n",
    "\n",
    "            for (min_p, max_p) in PRICE_RANGES:\n",
    "                print(f\"\\n\uD83D\uDCB5 Scanning Range: ${min_p:,} - ${max_p:,}\")\n",
    "                \n",
    "                # We use pagination on the filtered URL\n",
    "                # Note: We append a random sort or specific price filter logic if needed, \n",
    "                # but simple deep pagination works best for Properstar.\n",
    "                for i in range(1, pages_per_segment + 1):\n",
    "                    # Offset pagination to simulate deep diving into different price pockets\n",
    "                    current_page = i + (PRICE_RANGES.index((min_p, max_p)) * 20)\n",
    "                    url = f\"{base_url}?p={current_page}\"\n",
    "                    \n",
    "                    try:\n",
    "                        await page.goto(url, timeout=30000)\n",
    "                        await page.wait_for_load_state(\"domcontentloaded\")\n",
    "                        await page.evaluate(\"window.scrollBy(0, document.body.scrollHeight)\")\n",
    "                        await page.wait_for_timeout(1000)\n",
    "\n",
    "                        content = await page.content()\n",
    "                        soup = BeautifulSoup(content, 'html.parser')\n",
    "                        cards = soup.select('article, div[class*=\"item-adaptive\"]')\n",
    "                        \n",
    "                        if not cards: break # Stop segment if no more items\n",
    "\n",
    "                        for card in cards:\n",
    "                            try:\n",
    "                                # Get URL and Price\n",
    "                                title_tag = card.find('a', href=True)\n",
    "                                price_tag = card.find(string=re.compile(r'\\$'))\n",
    "                                \n",
    "                                if title_tag and price_tag:\n",
    "                                    link = f\"https://www.properstar.com{title_tag['href']}\"\n",
    "                                    price = float(re.sub(r'[^\\d.]', '', price_tag))\n",
    "                                    \n",
    "                                    # Basic Address Grab\n",
    "                                    addr = title_tag.get_text(strip=True)\n",
    "                                    \n",
    "                                    collected_data.append({\n",
    "                                        \"address\": f\"{addr}, New York, USA\",\n",
    "                                        \"price\": price,\n",
    "                                        \"url\": link\n",
    "                                    })\n",
    "                            except: continue\n",
    "                        \n",
    "                        print(f\"   PAGE {i}: Found {len(cards)} items.\")\n",
    "                    except: continue\n",
    "        finally:\n",
    "            await browser.close()\n",
    "            \n",
    "    return pd.DataFrame(collected_data)\n",
    "\n",
    "# Run Phase 1\n",
    "df_raw = asyncio.get_event_loop().run_until_complete(scrape_listing_urls())\n",
    "df_raw = df_raw.drop_duplicates(subset=['url'])\n",
    "print(f\"\\n\uD83C\uDF89 FOUND {len(df_raw)} LISTINGS.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0536a918-b10b-4e57-9a9c-eb6f1649ceef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Phase 2 - The \"Deep\" Scraper (Robust Batch Mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1bcf6503-55cf-42c4-a4c3-9bbba3b5d990",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "import random\n",
    "\n",
    "# File to save progress\n",
    "PROGRESS_FILE = \"nyc_enriched_progress.csv\"\n",
    "\n",
    "# Remove old file if starting fresh run\n",
    "if os.path.exists(PROGRESS_FILE): os.remove(PROGRESS_FILE)\n",
    "\n",
    "async def process_batch(batch_df):\n",
    "    results = []\n",
    "    # Rotate Session ID to avoid blocks\n",
    "    session_id = str(uuid.uuid4())[:8]\n",
    "    conn = f'wss://{BRIGHTDATA_USER}-session-{session_id}:{BRIGHTDATA_PASS}@{BRIGHTDATA_HOST}'\n",
    "    \n",
    "    async with async_playwright() as p:\n",
    "        try:\n",
    "            browser = await p.chromium.connect_over_cdp(conn)\n",
    "            context = await browser.new_context(locale='en-US')\n",
    "            page = await context.new_page()\n",
    "            \n",
    "            for _, row in batch_df.iterrows():\n",
    "                try:\n",
    "                    await page.goto(row['url'], timeout=40000)\n",
    "                    await page.wait_for_load_state(\"domcontentloaded\")\n",
    "                    \n",
    "                    html = await page.content()\n",
    "                    soup = BeautifulSoup(html, 'html.parser')\n",
    "                    text = soup.get_text(\" \", strip=True).lower()\n",
    "                    \n",
    "                    # Extraction Logic\n",
    "                    beds, baths, sqft = 0, 0, 0\n",
    "                    \n",
    "                    # Regex Patterns\n",
    "                    m_bed = re.search(r'(\\d+)\\s*(?:bed|bd|bedroom)', text)\n",
    "                    if m_bed: beds = int(m_bed.group(1))\n",
    "                    \n",
    "                    m_bath = re.search(r'(\\d+)\\s*(?:bath|ba)', text)\n",
    "                    if m_bath: baths = int(m_bath.group(1))\n",
    "                    \n",
    "                    m_sqft = re.search(r'(\\d+(?:,\\d+)?)\\s*sq', text)\n",
    "                    if m_sqft: sqft = int(m_sqft.group(1).replace(',', ''))\n",
    "                    \n",
    "                    results.append({\n",
    "                        **row.to_dict(),\n",
    "                        \"beds\": beds,\n",
    "                        \"baths\": baths,\n",
    "                        \"sqft\": sqft,\n",
    "                        \"amenities\": str(text[:500]) # Store snippet for analysis\n",
    "                    })\n",
    "                    print(f\"   ✅ Processed: {row['url'].split('/')[-1]}\")\n",
    "                except:\n",
    "                    results.append(row.to_dict()) # Keep row even if fail\n",
    "            await browser.close()\n",
    "        except: pass\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# BATCH RUNNER\n",
    "BATCH_SIZE = 50\n",
    "total = len(df_raw)\n",
    "\n",
    "print(f\"\uD83D\uDD75️ Phase 2: Deep Enriching {total} items (Saving every {BATCH_SIZE})...\")\n",
    "\n",
    "for i in range(0, total, BATCH_SIZE):\n",
    "    batch = df_raw.iloc[i : i + BATCH_SIZE]\n",
    "    print(f\"\\n\uD83D\uDD04 Batch {i}-{i+BATCH_SIZE}...\")\n",
    "    \n",
    "    df_batch = asyncio.get_event_loop().run_until_complete(process_batch(batch))\n",
    "    \n",
    "    # Save to disk\n",
    "    hdr = (i == 0)\n",
    "    df_batch.to_csv(PROGRESS_FILE, mode='a', header=hdr, index=False)\n",
    "\n",
    "print(\"\\n\uD83C\uDF89 Phase 2 Complete. Data saved to disk.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "1_scraper_properstar",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}